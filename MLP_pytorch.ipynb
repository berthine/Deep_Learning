{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multi-Layer Perceptron (MLP) with Pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Create Model Class \n",
    "- initialize the parameters of the model: input_dim, nb_hidden and the output_dim\n",
    "- Define the fc layers and the non-linearity function\n",
    "- define the forward : define how the output is computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are a define a simple neural network with 2 fc layer and one non-linearity function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model class :specify the input, the dim_hidden,output_dim\n",
    "#the non_linearity function doesn't affect the dimension of the data\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim,hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        #linear_function\n",
    "        self.fc1=nn.Linear(input_dim,hidden_dim)\n",
    "        #non-linearity\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #lin funct\n",
    "        out = self.fc1(x) #output of the 1st layer\n",
    "        #non-lin funct\n",
    "        out = self.tanh(out) # output of non_lin funct\n",
    "        #lin funct (readout)\n",
    "        out = self.fc2(out) #logits\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "    def __init__(self, input_dim,hidden_dim, output_dim):\n",
    "        super(MLP2, self).__init__()\n",
    "        #linear_function\n",
    "        self.fc1=nn.Linear(input_dim,hidden_dim)\n",
    "        self.fc2=nn.Linear(hidden_dim,hidden_dim)\n",
    "        #non-linearity\n",
    "        self.Relu = nn.ReLU() \n",
    "        self.fc3=nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #lin funct\n",
    "        out = self.fc1(x) #output of the 1st layer\n",
    "        out = self.fc2(out)\n",
    "        #non-lin funct\n",
    "        out = self.Relu(out) # output of non_lin funct # we can use F.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        #lin funct (readout)\n",
    "        out = self.fc4(out) #logits\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Dataset Loader\n",
    "* Loading the dataset : specify the root where the dataset is, load training and testing datasets, transforms, dowload=true if you want to dowload from pytorch\n",
    "* Data loader : Make the dataset iterable by define the batch_size, shuffle if you want to shuffle or not the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Loading Data\n",
    "    \n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "## Data Loader\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using th model MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28  #size of the image\n",
    "hidden_dim = 100   # number of neurons\n",
    "output_dim = 10  #Because we have 10 classes\n",
    "\n",
    "model = MLP(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Loss function\n",
    "\n",
    "Because we are going through a classification problem, cross entropy function is required to compute the loss between our softmax outputs and our binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Optimizer\n",
    "* Update the model's parameters at every iteration\n",
    "* We are using an optimization algorithm called Stochastic Gradient Descent (SGD) \n",
    "*torch.optim to see the other loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1 #Learning rate determines how fast the algorithm learns. \n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x7f1428c3fcd0>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# get the length of the list of t\n",
    "print(len(list(model.parameters())))  #return the number of layers\n",
    "print(list(model.parameters())[0].size()) # fc1 Parameters \n",
    "print(list(model.parameters())[1].size())  # fc1 bais Parameters\n",
    "print(list(model.parameters())[2].size())\n",
    "print(list(model.parameters())[3].size())  #the size of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Train Model\n",
    "* Process\n",
    "- Convert inputs to tensors with gradient accumulation capabilities\n",
    "- Clear gradient buffers\n",
    "- Get output given inputs : the forward\n",
    "- Get loss\n",
    "- Do backward\n",
    "- Update parameters using gradients \n",
    "- REPEAT the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.2778734862804413. Accuracy: 91\n",
      "Iteration: 1000. Loss: 0.19817772507667542. Accuracy: 92\n",
      "Iteration: 1500. Loss: 0.1722957193851471. Accuracy: 93\n",
      "Iteration: 2000. Loss: 0.383125901222229. Accuracy: 94\n",
      "Iteration: 2500. Loss: 0.1928856521844864. Accuracy: 94\n",
      "Iteration: 3000. Loss: 0.22457529604434967. Accuracy: 95\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model's state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/aims/Downloads/Transfer_learning/checkpoint.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), path)        #state_dict(): the dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aims/Downloads/Transfer_learning'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os \n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[-0.0046,  0.0006, -0.0235,  ..., -0.0025,  0.0009, -0.0252],\n",
       "                      [ 0.0318,  0.0326,  0.0221,  ..., -0.0277, -0.0340, -0.0222],\n",
       "                      [ 0.0122,  0.0229, -0.0033,  ..., -0.0071, -0.0286,  0.0008],\n",
       "                      ...,\n",
       "                      [ 0.0340, -0.0275,  0.0113,  ..., -0.0258, -0.0242,  0.0091],\n",
       "                      [ 0.0152,  0.0054, -0.0102,  ..., -0.0207, -0.0142, -0.0322],\n",
       "                      [-0.0226, -0.0002,  0.0327,  ...,  0.0048, -0.0144, -0.0341]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 8.9256e-03,  1.5081e-01,  1.3231e-01,  1.6254e-01,  1.6533e-01,\n",
       "                      -2.3301e-02,  1.1067e-01, -1.4146e-01,  7.6401e-02, -2.0366e-01,\n",
       "                      -1.2693e-01,  8.7816e-02,  2.1667e-01,  1.1925e-01,  1.0255e-01,\n",
       "                       1.2123e-01,  9.9078e-02, -7.2657e-02, -1.7623e-01,  1.6506e-01,\n",
       "                       2.4597e-02, -8.8836e-02, -2.4959e-01, -8.0441e-02,  5.1782e-02,\n",
       "                      -5.8986e-02,  1.3786e-02,  1.0226e-01, -3.6691e-02,  7.8001e-03,\n",
       "                      -7.7581e-04, -4.7499e-03, -5.5531e-02, -1.5869e-01, -1.0005e-02,\n",
       "                      -1.4722e-01, -2.0178e-02,  1.2016e-01, -1.5703e-01, -3.0304e-04,\n",
       "                       1.0879e-01,  1.6914e-02,  3.2451e-02, -3.8183e-04,  1.1361e-01,\n",
       "                       2.8779e-02,  1.3518e-02, -2.2135e-02,  1.1611e-01, -2.5641e-01,\n",
       "                      -1.4989e-01, -8.0953e-02, -1.1097e-01, -1.4891e-01, -9.0282e-03,\n",
       "                      -9.4963e-03,  7.1838e-02, -4.7358e-02, -1.4729e-02,  3.9810e-03,\n",
       "                      -9.4231e-02,  2.3121e-01, -1.7073e-01, -1.1917e-02,  9.5336e-02,\n",
       "                      -5.6403e-02,  2.0290e-01, -1.7969e-02,  1.4463e-01, -1.5753e-01,\n",
       "                       1.0328e-01,  2.7321e-02, -1.3745e-02, -1.0317e-01,  2.0673e-02,\n",
       "                      -5.9287e-02, -5.8116e-02, -3.2438e-03,  5.0006e-02, -3.3969e-02,\n",
       "                       4.9478e-02, -1.1553e-01, -1.0192e-01,  8.6533e-02,  1.3425e-01,\n",
       "                      -6.0361e-02, -1.4606e-01, -4.2395e-02, -1.4563e-01,  1.9740e-01,\n",
       "                       3.6487e-03,  3.5221e-02, -2.3500e-04, -7.6323e-03,  1.2699e-02,\n",
       "                      -9.2330e-02,  1.5322e-01, -1.1785e-01,  7.1879e-02, -7.0342e-02])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-2.1607e-01, -2.1215e-01, -3.7242e-01,  2.7135e-01, -1.1264e-01,\n",
       "                       -1.4405e-01, -1.3809e-01,  8.3713e-02,  1.1411e-01,  4.7217e-01,\n",
       "                        1.9678e-01, -2.3521e-01, -1.8709e-01,  2.3830e-01, -1.3376e-01,\n",
       "                       -4.3681e-01, -1.9006e-01, -2.3150e-01,  3.1076e-01,  7.1178e-02,\n",
       "                       -3.2587e-01,  4.8597e-03,  5.8007e-02, -1.1513e-01, -1.2448e-01,\n",
       "                       -3.8770e-01, -1.3461e-01, -2.1021e-01,  2.9050e-02, -7.2485e-03,\n",
       "                        2.2218e-01,  2.8430e-01, -3.5865e-01, -1.8161e-01,  4.9076e-02,\n",
       "                        2.8604e-01,  4.3008e-01, -1.5393e-01, -1.0974e-02,  1.4972e-01,\n",
       "                       -1.2258e-01, -1.5974e-01,  5.3474e-02,  1.2538e-01, -1.7440e-01,\n",
       "                       -1.3928e-01, -4.0871e-01,  1.1129e-01,  1.9839e-01, -8.3515e-02,\n",
       "                        1.6750e-01, -3.1324e-01, -3.2473e-01,  1.1348e-01,  4.7540e-02,\n",
       "                       -3.2804e-01,  5.8847e-02, -2.2765e-01,  3.7378e-01,  7.3388e-02,\n",
       "                       -2.2782e-01, -2.2924e-02,  3.0731e-01, -3.6865e-01,  4.3211e-02,\n",
       "                        2.6624e-01, -2.2885e-01,  2.5321e-01, -2.0539e-02,  1.9919e-02,\n",
       "                       -2.2223e-01,  1.5688e-01, -2.9205e-01,  2.0792e-01, -3.3390e-01,\n",
       "                       -4.4091e-02, -1.8802e-01, -3.1943e-01, -3.1740e-01,  1.4085e-01,\n",
       "                        1.7836e-01, -1.6273e-01,  8.0424e-02, -1.4983e-01,  1.3388e-01,\n",
       "                       -2.0611e-02, -9.5517e-02, -7.3316e-02, -1.9853e-01, -2.2854e-01,\n",
       "                        6.7071e-02, -4.2213e-03, -4.2116e-02, -1.1361e-01,  4.3352e-01,\n",
       "                       -7.1209e-02, -2.2224e-01, -1.6405e-01, -4.3641e-01,  2.6463e-01],\n",
       "                      [-3.6349e-02,  2.4370e-01,  1.4658e-01, -2.8098e-01,  3.1139e-01,\n",
       "                        4.0980e-01, -2.4574e-01, -2.8192e-01,  1.9154e-01, -2.3087e-01,\n",
       "                        2.5088e-01, -8.3885e-02,  1.6273e-01,  2.2937e-01, -1.3347e-02,\n",
       "                        4.5171e-01,  4.2373e-01, -3.3618e-02,  2.2642e-01, -3.4863e-01,\n",
       "                       -2.1462e-01, -2.2012e-01, -6.0469e-03, -1.1672e-02,  2.4298e-01,\n",
       "                        3.5652e-01,  2.1051e-01, -7.2740e-04, -1.0599e-01,  2.6355e-01,\n",
       "                       -3.2084e-01,  1.9714e-03, -3.7995e-01, -9.3465e-02, -1.7815e-01,\n",
       "                        9.2967e-02,  1.7756e-02,  9.6867e-02, -1.6906e-01, -1.4855e-01,\n",
       "                        1.3917e-01,  1.3888e-02, -2.4588e-01, -6.1582e-02,  3.1511e-01,\n",
       "                        4.9978e-01,  3.9769e-02, -2.8241e-01,  2.1560e-01, -2.7351e-03,\n",
       "                        1.1306e-01,  3.7129e-02, -1.8834e-01, -1.0725e-01, -4.4312e-01,\n",
       "                        2.3161e-01, -2.8049e-01,  1.8332e-01, -1.1227e-01,  1.3739e-01,\n",
       "                        2.4739e-01,  2.7891e-01, -2.0952e-01, -2.9450e-01, -3.5686e-04,\n",
       "                        2.0523e-01, -2.9894e-01,  1.1913e-01,  1.0624e-01,  2.3436e-01,\n",
       "                        3.3912e-01,  7.8403e-02,  1.2095e-01, -2.9602e-01, -4.4880e-02,\n",
       "                        6.6126e-02,  1.1682e-01,  1.9940e-01,  1.5851e-01, -1.8373e-01,\n",
       "                       -3.2029e-01,  2.3544e-01,  1.5144e-01,  5.2125e-02,  2.4732e-01,\n",
       "                        2.7436e-01,  3.3107e-01,  2.4934e-01, -1.2205e-01,  2.6788e-01,\n",
       "                        1.6024e-01, -3.0970e-01, -3.1992e-01,  3.6907e-01,  4.8726e-02,\n",
       "                       -1.7271e-01,  3.5133e-02, -2.9767e-01,  1.5265e-01, -2.3136e-01],\n",
       "                      [-1.7621e-01, -4.8725e-01, -3.0754e-01,  1.2433e-01,  2.8775e-01,\n",
       "                        1.4848e-01,  2.1823e-01,  1.2338e-01,  8.2803e-02,  2.9532e-02,\n",
       "                       -3.6835e-01,  1.0882e-01, -2.2137e-01, -2.3367e-01, -4.9044e-01,\n",
       "                        1.6308e-01, -8.5767e-02,  4.3589e-01,  6.1378e-02, -1.7786e-01,\n",
       "                        2.0547e-01, -8.2115e-02,  1.5153e-01,  1.3113e-01, -2.8459e-01,\n",
       "                       -1.0523e-01,  2.2731e-01,  3.4546e-02,  1.5755e-01, -1.4079e-01,\n",
       "                       -3.0000e-01, -3.2123e-01,  6.1835e-02,  2.6977e-01,  2.4682e-01,\n",
       "                        2.6123e-01, -6.8167e-02,  1.4561e-01,  3.8411e-01,  3.9316e-01,\n",
       "                        1.3619e-01,  3.4445e-01,  6.3784e-01,  6.8744e-04, -2.9856e-01,\n",
       "                        3.8613e-01,  1.8283e-01,  2.4518e-01,  2.9942e-01, -3.5951e-01,\n",
       "                        4.2655e-01, -6.6462e-01, -2.5860e-01,  9.0903e-02, -2.7747e-01,\n",
       "                       -3.2810e-01, -1.5413e-01,  1.7259e-01, -2.8325e-01,  2.7833e-01,\n",
       "                        3.1100e-02,  2.0797e-01, -5.6629e-02,  1.2104e-01,  1.6585e-01,\n",
       "                       -1.6368e-01, -1.7516e-01,  2.4994e-01, -2.5096e-01, -1.1016e-01,\n",
       "                       -3.1408e-01,  2.0078e-01,  2.2812e-01, -1.5730e-01, -2.6716e-01,\n",
       "                       -2.3684e-01, -8.5564e-02,  2.7725e-01,  1.6047e-01,  2.4038e-01,\n",
       "                        1.5846e-01,  7.1418e-02,  2.2981e-01,  2.9254e-01, -3.5419e-01,\n",
       "                       -2.4090e-02,  3.9854e-01,  3.7385e-01, -3.7367e-01,  1.1900e-01,\n",
       "                       -1.8125e-01,  4.2459e-01,  5.5171e-01,  8.1428e-02,  3.3308e-01,\n",
       "                        1.6934e-01,  1.1807e-01,  3.9377e-01,  1.0794e-01,  3.5277e-01],\n",
       "                      [-5.4484e-02, -2.3473e-01,  1.0254e-02, -2.0727e-01, -3.7839e-01,\n",
       "                       -1.2431e-01, -1.2681e-01, -1.4569e-01,  4.0689e-01, -1.9426e-01,\n",
       "                        1.3781e-01,  8.6225e-02,  1.0088e-01, -8.9543e-02,  1.8923e-01,\n",
       "                        1.1975e-01,  3.6063e-01,  3.4511e-01,  1.1244e-01, -3.7878e-01,\n",
       "                       -4.3131e-01,  5.6724e-03, -2.5971e-01, -3.6935e-01, -2.2307e-01,\n",
       "                        1.2978e-01, -1.4805e-01,  4.5540e-03,  2.7767e-01,  2.6992e-03,\n",
       "                       -3.2966e-01, -3.3107e-02,  9.7632e-03,  1.2130e-01,  5.7774e-03,\n",
       "                        8.9312e-02, -3.6379e-01,  2.2544e-01, -3.3550e-01, -2.1625e-02,\n",
       "                        1.1939e-01,  1.5464e-01,  1.6107e-01,  1.9478e-01,  3.9060e-01,\n",
       "                       -1.9803e-02, -2.8239e-02,  2.7227e-01, -1.4247e-01,  2.6670e-01,\n",
       "                        2.2047e-01, -2.5606e-01,  8.4927e-02, -1.7969e-01,  1.6838e-01,\n",
       "                        2.4101e-01, -1.7458e-01, -2.9539e-01,  1.1989e-01,  2.0323e-01,\n",
       "                        3.3955e-01, -1.5211e-01, -1.2251e-01,  8.2555e-02, -1.8256e-01,\n",
       "                       -4.2389e-01,  1.3567e-01, -4.1849e-01, -2.8701e-01,  1.5713e-01,\n",
       "                        3.4238e-01,  1.5236e-01,  1.1917e-01,  2.0597e-01,  4.0909e-01,\n",
       "                       -3.3047e-01, -2.8621e-01,  1.9739e-01, -1.9976e-01,  1.3575e-01,\n",
       "                        1.8238e-01, -2.5416e-01,  3.2651e-03,  7.1329e-02, -5.5126e-01,\n",
       "                       -2.4563e-01, -3.6292e-01, -3.7306e-02, -2.7255e-01,  2.0871e-01,\n",
       "                        2.5576e-01, -1.8376e-01,  4.0237e-01,  5.1960e-01, -3.5064e-01,\n",
       "                        2.6044e-01,  1.9554e-01,  2.2667e-02,  3.5376e-01,  2.3605e-01],\n",
       "                      [ 9.0253e-02,  1.3858e-01, -2.2967e-01,  1.6522e-02, -1.8361e-01,\n",
       "                        4.6950e-01, -2.9915e-01,  3.5420e-01, -9.5027e-02,  4.3215e-03,\n",
       "                       -2.8175e-01,  4.6192e-01,  3.0627e-02, -3.0154e-01,  9.6938e-02,\n",
       "                       -4.7187e-01, -1.8993e-01, -3.0467e-01, -3.4845e-01,  9.4594e-02,\n",
       "                        2.2427e-01,  3.1761e-01, -1.0722e-01,  3.2855e-01,  5.5418e-01,\n",
       "                       -3.8227e-03, -3.5897e-02, -2.6817e-01, -3.1296e-02, -3.5240e-01,\n",
       "                        2.7741e-01, -3.0799e-01, -2.7409e-01,  2.3906e-01, -3.4993e-01,\n",
       "                        1.2456e-02, -5.0346e-01, -1.7476e-01,  2.3411e-01, -4.9781e-01,\n",
       "                       -8.1940e-02, -2.3455e-01, -3.0634e-01, -1.5291e-01, -2.0253e-01,\n",
       "                       -2.3032e-01,  1.4108e-01, -3.7212e-01,  1.5077e-01,  1.8357e-01,\n",
       "                       -2.5378e-01,  1.8399e-01,  2.1884e-01, -1.5580e-01, -1.4925e-01,\n",
       "                       -4.2904e-02,  3.0074e-01,  1.0628e-01, -1.3381e-01, -8.1102e-02,\n",
       "                       -2.5872e-01,  8.7101e-02, -6.4837e-02,  2.5564e-01,  3.5132e-01,\n",
       "                       -1.8914e-01,  2.7306e-01, -2.0457e-01,  1.8271e-01, -6.4717e-02,\n",
       "                       -3.5809e-02, -7.7499e-02,  1.5432e-01,  1.8385e-01, -2.3237e-03,\n",
       "                        2.5639e-01,  1.0227e-01,  1.5987e-01,  5.6804e-01, -1.4538e-01,\n",
       "                        2.8238e-01, -1.5589e-01, -3.8705e-01, -3.5440e-01,  1.6151e-01,\n",
       "                       -1.2699e-01, -2.6059e-01, -1.4366e-01,  3.7224e-01,  1.8556e-02,\n",
       "                       -1.5271e-01,  2.1704e-01,  1.8483e-01, -1.4137e-01,  7.9525e-02,\n",
       "                        1.8307e-01, -2.9159e-01,  2.9986e-01,  4.4584e-02, -2.8490e-01],\n",
       "                      [-1.2495e-01,  4.7274e-01,  2.5939e-01,  6.1395e-01,  2.9943e-01,\n",
       "                       -4.7051e-01,  1.3759e-01, -3.1800e-01, -1.0956e-01, -4.8102e-01,\n",
       "                       -4.6289e-02,  1.6013e-01,  1.4088e-01, -3.5642e-03,  1.4490e-01,\n",
       "                        2.4695e-01, -1.3307e-01,  6.1261e-02, -1.3054e-01,  5.4772e-01,\n",
       "                        9.1405e-02,  1.9508e-01, -7.1645e-01, -4.5872e-01,  1.7289e-01,\n",
       "                        9.3988e-02,  5.2403e-02,  6.6879e-02, -2.2557e-01, -9.4449e-02,\n",
       "                       -7.8494e-02,  5.1530e-01,  2.2346e-01, -5.4264e-01,  2.8772e-01,\n",
       "                       -5.6498e-01,  3.0122e-01,  3.0208e-01, -2.3802e-01,  4.8775e-01,\n",
       "                        2.0943e-01,  3.2378e-01, -2.6073e-01,  6.3504e-02, -1.1099e-01,\n",
       "                       -1.9513e-01,  2.9533e-01,  2.1202e-02,  1.4101e-01, -8.9736e-02,\n",
       "                       -5.4519e-01, -4.4941e-01, -1.9784e-01,  1.0043e-01,  4.2816e-01,\n",
       "                       -2.1685e-01,  1.0553e-01,  1.4757e-01, -1.9593e-01, -1.4433e-01,\n",
       "                       -1.5518e-01,  2.6809e-01, -3.3890e-01, -4.4508e-01, -1.3306e-01,\n",
       "                        1.5336e-01,  4.5323e-01, -2.0898e-01,  5.6951e-02, -2.1919e-01,\n",
       "                        2.6038e-01, -4.0919e-02,  1.0038e-01, -1.1002e-01,  1.9805e-01,\n",
       "                        3.4671e-01, -6.4785e-02, -1.2350e-01, -2.1251e-02, -2.6910e-01,\n",
       "                        3.6883e-01, -4.2229e-01, -1.1260e-01, -6.2764e-02,  3.4461e-01,\n",
       "                        2.4377e-01, -3.5376e-01, -1.7434e-01, -4.5727e-01,  2.9020e-02,\n",
       "                       -3.1774e-01, -2.7244e-01, -6.6005e-02, -4.9072e-01, -1.3775e-01,\n",
       "                       -1.5107e-01,  2.5696e-01, -6.5692e-01,  2.9151e-01,  1.6010e-01],\n",
       "                      [ 3.0938e-02, -2.4328e-01,  2.4049e-01,  1.6457e-02, -1.4011e-01,\n",
       "                       -1.4805e-01,  7.0926e-02,  4.6743e-01, -3.5668e-01,  1.8116e-01,\n",
       "                        2.6593e-01, -8.1038e-02,  2.2785e-02, -2.9155e-01, -7.9975e-02,\n",
       "                        4.3882e-01, -6.5969e-02, -1.8513e-01,  1.2069e-01,  2.3399e-01,\n",
       "                       -4.0324e-01,  2.0588e-01,  2.6680e-01,  3.1136e-01,  3.1047e-02,\n",
       "                       -1.6065e-01,  1.5213e-01, -9.0642e-02, -1.4402e-01,  1.2060e-02,\n",
       "                        4.1266e-01, -5.5188e-02,  5.4182e-02, -2.0842e-02,  1.3776e-01,\n",
       "                       -9.7204e-02, -1.2505e-01,  2.1084e-01,  5.3934e-02,  2.8608e-01,\n",
       "                       -2.9607e-01, -1.4539e-01,  2.2980e-01,  1.2795e-01, -3.1343e-01,\n",
       "                        1.7424e-01,  1.9654e-01, -4.9382e-01, -2.8843e-01,  1.2149e-01,\n",
       "                        3.2150e-02,  2.8416e-01, -5.0284e-01, -1.4222e-01,  1.4626e-01,\n",
       "                       -3.6230e-01,  2.6491e-01,  3.1102e-01,  1.1447e-01,  5.3291e-02,\n",
       "                        3.3891e-01,  1.3179e-01,  3.0555e-01,  1.4439e-02,  4.3631e-02,\n",
       "                       -2.8010e-01, -2.9106e-01, -7.5190e-02,  2.9442e-01,  9.6701e-02,\n",
       "                       -1.1187e-01,  3.9763e-01, -2.8151e-01, -8.2294e-02, -4.2503e-01,\n",
       "                        2.0910e-01, -2.1977e-01, -7.3878e-02,  4.8886e-01, -2.3396e-01,\n",
       "                       -3.6471e-01,  3.6787e-01, -8.8023e-02, -3.3283e-01,  3.8430e-01,\n",
       "                        1.7932e-01,  3.6370e-01,  1.3398e-01,  2.6104e-01, -3.5912e-01,\n",
       "                       -1.0501e-01,  2.1878e-01,  1.0358e-01,  7.0358e-02,  1.0334e-01,\n",
       "                       -5.1998e-01, -1.7746e-02, -2.4735e-01,  6.1376e-02, -3.1474e-01],\n",
       "                      [ 3.3121e-01,  1.8580e-01,  1.8479e-01, -6.6803e-02,  1.5392e-01,\n",
       "                       -8.1090e-02,  2.4549e-01, -2.6293e-01,  2.1920e-01, -1.6890e-01,\n",
       "                       -2.7623e-01, -1.2097e-01,  4.5578e-01,  5.9058e-01,  3.8558e-01,\n",
       "                        1.0573e-01,  1.2818e-01,  3.7643e-02, -3.8270e-01, -1.2255e-01,\n",
       "                        2.1440e-01, -2.9887e-01,  4.4461e-02, -9.3854e-03, -2.7356e-01,\n",
       "                       -3.8883e-01, -1.0046e-01,  3.4582e-01,  2.7058e-02,  2.6319e-01,\n",
       "                       -3.5547e-01, -9.7864e-02, -2.0833e-01, -1.2245e-01, -3.4843e-01,\n",
       "                        2.1289e-01,  1.7354e-01, -2.7563e-01, -2.6555e-01, -4.6691e-01,\n",
       "                       -3.0142e-01,  2.0564e-01,  1.6475e-01, -2.7626e-01,  5.4006e-01,\n",
       "                       -2.0895e-01, -4.3858e-01,  2.4444e-01, -1.5314e-01, -5.5472e-01,\n",
       "                        1.5529e-02,  3.1119e-01,  4.4883e-01, -4.1682e-01,  3.4228e-03,\n",
       "                        3.4008e-01,  2.2599e-01, -3.1124e-01,  5.1886e-01,  8.3802e-02,\n",
       "                       -2.3233e-01, -2.1361e-01, -2.0716e-01,  4.4802e-01,  2.8173e-01,\n",
       "                       -2.9697e-02,  3.2230e-01,  1.8551e-01,  9.9262e-02, -3.3164e-01,\n",
       "                       -3.5728e-01, -2.2769e-01, -4.6168e-01, -1.8630e-01,  8.6909e-02,\n",
       "                       -4.0103e-01,  1.3345e-01, -2.6282e-01, -2.0272e-01,  1.7026e-01,\n",
       "                       -2.8117e-01, -2.4962e-01, -5.6780e-01, -8.9048e-02,  7.4690e-02,\n",
       "                       -2.9579e-01,  1.3293e-01, -5.3187e-03,  1.0715e-01,  3.2065e-01,\n",
       "                        2.8248e-01,  2.4350e-01, -1.2076e-02, -8.2042e-02, -7.9338e-02,\n",
       "                        2.0189e-01,  3.9429e-02,  1.7764e-01, -1.4248e-01,  1.4283e-01],\n",
       "                      [-4.3099e-02, -1.3532e-01, -7.5390e-02, -1.4253e-01, -2.4983e-01,\n",
       "                       -1.6091e-01, -1.8089e-01, -1.6408e-02, -2.4481e-01,  9.9221e-02,\n",
       "                        2.0464e-01, -2.1748e-01, -1.5322e-01, -8.5740e-02, -4.9142e-02,\n",
       "                       -2.6309e-01, -1.9338e-01,  2.1762e-02,  3.6950e-01, -8.5033e-02,\n",
       "                        3.2927e-01,  1.2924e-01,  3.6014e-01,  1.1757e-01, -3.9688e-01,\n",
       "                        2.8856e-01,  1.0876e-01, -1.8774e-02, -1.4026e-01,  1.0299e-02,\n",
       "                       -1.8134e-02,  2.8221e-01,  4.5261e-01, -1.8884e-02,  2.4791e-01,\n",
       "                       -2.2881e-01,  3.8775e-01, -2.0462e-02,  2.7221e-01,  4.5040e-01,\n",
       "                       -1.5913e-01,  1.5992e-01, -1.3320e-01, -4.7389e-02, -1.0108e-01,\n",
       "                       -2.6268e-01,  3.2883e-01, -3.1528e-02, -1.0277e-01,  3.0546e-01,\n",
       "                       -1.3465e-02,  6.2698e-01,  3.5959e-01,  2.0482e-01,  1.9707e-01,\n",
       "                        1.0435e-01, -3.7318e-01,  2.4481e-01, -2.4180e-01, -1.3971e-01,\n",
       "                        1.3627e-01, -2.4203e-01, -1.0058e-01,  5.5308e-02, -4.1939e-01,\n",
       "                        4.1664e-01, -3.1548e-01,  3.0568e-01, -4.4297e-01,  1.8526e-01,\n",
       "                       -2.0628e-01, -2.7877e-01,  2.4340e-01,  1.2617e-01,  1.1354e-02,\n",
       "                        2.6366e-01,  3.3801e-01,  1.8168e-01, -2.7710e-01, -1.6214e-01,\n",
       "                       -2.4689e-01,  1.1353e-01,  3.0105e-01, -1.8700e-01, -5.1594e-01,\n",
       "                        3.3105e-01,  2.6198e-01, -9.0975e-02,  3.9718e-01, -1.9950e-01,\n",
       "                       -3.3254e-01, -4.4463e-01,  4.8834e-02, -3.2757e-01, -1.3947e-01,\n",
       "                        1.8125e-01, -8.7785e-03,  6.5464e-01,  1.0878e-02,  2.7303e-01],\n",
       "                      [ 1.2162e-01,  3.3431e-01,  1.0780e-01, -3.7463e-01,  1.2888e-01,\n",
       "                       -2.1238e-01,  1.4215e-01, -9.0072e-02, -3.4057e-01, -6.6803e-02,\n",
       "                        7.8645e-02, -1.7518e-01,  1.3634e-02, -1.2889e-01,  1.6713e-01,\n",
       "                       -3.6576e-01, -1.9735e-01, -4.7344e-01, -1.0577e-01,  2.5193e-01,\n",
       "                        3.6366e-01, -3.2787e-01,  3.7881e-01, -1.1974e-02,  4.2455e-01,\n",
       "                       -2.3313e-01, -3.4787e-01, -1.6000e-01,  2.3297e-01,  2.6968e-01,\n",
       "                        4.5463e-01, -2.1840e-02,  3.3331e-01,  1.3048e-01, -1.9628e-01,\n",
       "                        1.4530e-01, -1.4442e-01, -1.3103e-01,  1.3318e-01, -4.9012e-01,\n",
       "                        1.8554e-01, -4.8727e-01, -2.8915e-01, -1.7365e-01, -1.1960e-01,\n",
       "                        5.4162e-02, -3.4821e-01,  2.9671e-01, -1.2778e-01,  1.9174e-01,\n",
       "                       -1.6950e-01,  3.5791e-02,  2.0613e-01,  3.7462e-01, -7.6416e-02,\n",
       "                        2.4270e-01, -2.7528e-02, -1.4564e-01, -1.7387e-01, -3.5667e-01,\n",
       "                       -3.8189e-01, -3.1670e-01, -1.2682e-02,  2.2679e-01, -2.1091e-01,\n",
       "                       -9.4108e-04,  2.1339e-01,  1.4012e-01,  2.9409e-01, -6.4974e-02,\n",
       "                        1.3714e-01, -3.9201e-01,  1.2932e-01,  2.3904e-01,  8.6418e-02,\n",
       "                        8.9651e-02, -1.0755e-01, -1.3567e-01, -6.6250e-01,  2.9748e-01,\n",
       "                        2.8987e-01,  4.3677e-01,  2.6599e-01,  5.1662e-01,  3.1675e-01,\n",
       "                       -3.0424e-01, -3.1209e-01, -3.7028e-01,  4.9134e-01, -6.9402e-02,\n",
       "                        2.1405e-01,  2.1796e-01, -5.4763e-01,  7.7275e-02, -2.4771e-01,\n",
       "                        1.0580e-01,  5.8296e-02,  1.0752e-01,  2.4993e-02, -2.0258e-01]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([-0.1630,  0.1828,  0.1858, -0.1063,  0.0153,  0.5558, -0.0228,  0.2099,\n",
       "                      -0.5703, -0.0492]))])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the model to checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* resquires_grad: the possibility to compute the gradient to the parameters\n",
    "* optimizer.zero_grad() : clear gradient\n",
    "* outputs = model(images): compute the forward\n",
    "* loss.backward() : compute backprogation of the loss respect to the parameters\n",
    "* optimizer.step(): update the parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Using medel MPL2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.30891722440719604. Accuracy: 91\n",
      "Iteration: 1000. Loss: 0.23510482907295227. Accuracy: 93\n",
      "Iteration: 1500. Loss: 0.22717426717281342. Accuracy: 94\n",
      "Iteration: 2000. Loss: 0.10250414907932281. Accuracy: 95\n",
      "Iteration: 2500. Loss: 0.12175756692886353. Accuracy: 95\n",
      "Iteration: 3000. Loss: 0.2126990705728531. Accuracy: 96\n"
     ]
    }
   ],
   "source": [
    "model_1 = MLP2(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1 #Learning rate determines how fast the algorithm learns. \n",
    "\n",
    "optimizer = torch.optim.SGD(model_1.parameters(), lr=learning_rate) \n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model_1(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model_1(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see with more layers we got a high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Arch\n",
    "2. dataset and dataloader\n",
    "3. Train and evaluation\n",
    "4. parameter tuning\n",
    "5. save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
